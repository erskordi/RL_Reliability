{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25837988",
   "metadata": {},
   "source": [
    "# Training agent using DDPG\n",
    "\n",
    "Model-free, off-policy RL method.\n",
    "\n",
    "Two networks here:\n",
    " - Actor: Proposes an action\n",
    " - Critic: Evaluates action based on new state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd45d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a128141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15565b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a549174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the Actor and Critic networks. \n",
    "\n",
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 10.0 for x.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52efd382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an action sampled from our Actor network plus some noise for exploration.\n",
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return np.asarray([np.squeeze(legal_action)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343808b7",
   "metadata": {},
   "source": [
    "## Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe110a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep import DataPrep\n",
    "from env import CMAPSSEnv\n",
    "from VAE_dense import VAE\n",
    "\n",
    "\n",
    "file_path = \"CMAPSSData/train_FD002.txt\"\n",
    "num_settings = 3\n",
    "num_sensors = 21\n",
    "num_units = 20\n",
    "step = \"RL\"\n",
    "\n",
    "neurons = [64, 32, 16, 8]\n",
    "\n",
    "# Data prep\n",
    "data = DataPrep(file=file_path,\n",
    "                num_settings=num_settings, \n",
    "                num_sensors=num_sensors, \n",
    "                num_units=num_units, \n",
    "                step=step,\n",
    "                normalization_type=\"01\")\n",
    "\n",
    "df = data.ReadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "833735f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  25\n",
      "Size of Action Space ->  1\n",
      "10.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# List of engine lifetimes\n",
    "engine_lives = df.groupby(df['Unit']).size().tolist()\n",
    "num_engines = len(engine_lives)\n",
    "\n",
    "# Load decoder\n",
    "vae = VAE(latent_dim=1,image_size=25)\n",
    "\n",
    "##########################################\n",
    "env_config = {\n",
    "    \"df\": df,\n",
    "    \"timestep\": 0,\n",
    "    \"obs_size\": num_settings+num_sensors+1,\n",
    "    \"engines\": num_engines,\n",
    "    \"engine_lives\": engine_lives, \n",
    "    \"decoder_model\": vae.load_models(),\n",
    "}\n",
    "\n",
    "env = CMAPSSEnv(**env_config)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(upper_bound, lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d8256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.9\n",
    "# Used to update target networks\n",
    "tau = 0.01\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65595cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -2102.74935068826\n",
      "Episode * 1 * Avg Reward is ==> -1989.5422561468686\n",
      "Episode * 2 * Avg Reward is ==> -1829.2923594417214\n",
      "Episode * 3 * Avg Reward is ==> -1834.8503166376051\n",
      "Episode * 4 * Avg Reward is ==> -1842.9452553944088\n",
      "Episode * 5 * Avg Reward is ==> -1834.4170466624744\n",
      "Episode * 6 * Avg Reward is ==> -1833.453530624976\n",
      "Episode * 7 * Avg Reward is ==> -1821.2922249463054\n",
      "Episode * 8 * Avg Reward is ==> -1854.9315885762594\n",
      "Episode * 9 * Avg Reward is ==> -1849.6623012147156\n",
      "Episode * 10 * Avg Reward is ==> -1815.7903894215124\n",
      "Episode * 11 * Avg Reward is ==> -1785.1420110554257\n",
      "Episode * 12 * Avg Reward is ==> -1749.143539494654\n",
      "Episode * 13 * Avg Reward is ==> -1723.582241311219\n",
      "Episode * 14 * Avg Reward is ==> -1707.8238654182985\n",
      "Episode * 15 * Avg Reward is ==> -1683.3079207101707\n",
      "Episode * 16 * Avg Reward is ==> -1669.8896622756783\n",
      "Episode * 17 * Avg Reward is ==> -1655.5262793822267\n",
      "Episode * 18 * Avg Reward is ==> -1639.9396506129385\n",
      "Episode * 19 * Avg Reward is ==> -1633.1899699492756\n",
      "Episode * 20 * Avg Reward is ==> -1622.92494375872\n",
      "Episode * 21 * Avg Reward is ==> -1610.6553851151875\n",
      "Episode * 22 * Avg Reward is ==> -1609.342961820359\n",
      "Episode * 23 * Avg Reward is ==> -1606.5560118862031\n",
      "Episode * 24 * Avg Reward is ==> -1602.302903304268\n",
      "Episode * 25 * Avg Reward is ==> -1594.2526676274038\n",
      "Episode * 26 * Avg Reward is ==> -1587.2584755140153\n",
      "Episode * 27 * Avg Reward is ==> -1577.6850864595428\n",
      "Episode * 28 * Avg Reward is ==> -1574.7360057702972\n",
      "Episode * 29 * Avg Reward is ==> -1574.684277405253\n",
      "Episode * 30 * Avg Reward is ==> -1569.0665374202704\n",
      "Episode * 31 * Avg Reward is ==> -1567.0601784444418\n",
      "Episode * 32 * Avg Reward is ==> -1561.9764505147245\n",
      "Episode * 33 * Avg Reward is ==> -1562.0823167572694\n",
      "Episode * 34 * Avg Reward is ==> -1555.2295408489074\n",
      "Episode * 35 * Avg Reward is ==> -1549.7335536492842\n",
      "Episode * 36 * Avg Reward is ==> -1547.4227207492738\n",
      "Episode * 37 * Avg Reward is ==> -1551.9924896791422\n",
      "Episode * 38 * Avg Reward is ==> -1548.8481942662386\n",
      "Episode * 39 * Avg Reward is ==> -1543.9559233687364\n",
      "Episode * 40 * Avg Reward is ==> -1528.354811278281\n",
      "Episode * 41 * Avg Reward is ==> -1515.1015807262893\n",
      "Episode * 42 * Avg Reward is ==> -1510.283710922154\n",
      "Episode * 43 * Avg Reward is ==> -1497.017362121484\n",
      "Episode * 44 * Avg Reward is ==> -1489.0083772276002\n",
      "Episode * 45 * Avg Reward is ==> -1481.021111450335\n",
      "Episode * 46 * Avg Reward is ==> -1469.0680304225186\n",
      "Episode * 47 * Avg Reward is ==> -1459.2677899075006\n",
      "Episode * 48 * Avg Reward is ==> -1442.0022966078502\n",
      "Episode * 49 * Avg Reward is ==> -1440.3314367160547\n",
      "Episode * 50 * Avg Reward is ==> -1440.5766678757511\n",
      "Episode * 51 * Avg Reward is ==> -1439.5850199515398\n",
      "Episode * 52 * Avg Reward is ==> -1439.8967556688463\n",
      "Episode * 53 * Avg Reward is ==> -1442.5381295157426\n",
      "Episode * 54 * Avg Reward is ==> -1442.5114647998446\n",
      "Episode * 55 * Avg Reward is ==> -1452.5190702098494\n",
      "Episode * 56 * Avg Reward is ==> -1451.3958086127714\n",
      "Episode * 57 * Avg Reward is ==> -1449.2046782513662\n",
      "Episode * 58 * Avg Reward is ==> -1449.710596737373\n",
      "Episode * 59 * Avg Reward is ==> -1446.5189393868666\n",
      "Episode * 60 * Avg Reward is ==> -1445.5333006590804\n",
      "Episode * 61 * Avg Reward is ==> -1449.1981460186184\n",
      "Episode * 62 * Avg Reward is ==> -1454.1137472204769\n",
      "Episode * 63 * Avg Reward is ==> -1448.7341750978721\n",
      "Episode * 64 * Avg Reward is ==> -1445.245507462609\n",
      "Episode * 65 * Avg Reward is ==> -1452.8202385925197\n",
      "Episode * 66 * Avg Reward is ==> -1452.4951100887156\n",
      "Episode * 67 * Avg Reward is ==> -1454.634246112398\n",
      "Episode * 68 * Avg Reward is ==> -1454.664037498587\n",
      "Episode * 69 * Avg Reward is ==> -1448.8904778569856\n",
      "Episode * 70 * Avg Reward is ==> -1452.862536897825\n",
      "Episode * 71 * Avg Reward is ==> -1456.4760493967128\n",
      "Episode * 72 * Avg Reward is ==> -1463.1477288738638\n",
      "Episode * 73 * Avg Reward is ==> -1469.062544855704\n",
      "Episode * 74 * Avg Reward is ==> -1472.0126487139928\n",
      "Episode * 75 * Avg Reward is ==> -1485.2927326612219\n",
      "Episode * 76 * Avg Reward is ==> -1486.9172763730814\n",
      "Episode * 77 * Avg Reward is ==> -1489.505713546698\n",
      "Episode * 78 * Avg Reward is ==> -1489.2898004335273\n",
      "Episode * 79 * Avg Reward is ==> -1489.4428133071829\n",
      "Episode * 80 * Avg Reward is ==> -1486.513078759006\n",
      "Episode * 81 * Avg Reward is ==> -1488.817165929639\n",
      "Episode * 82 * Avg Reward is ==> -1493.593873170073\n",
      "Episode * 83 * Avg Reward is ==> -1495.1959619698405\n",
      "Episode * 84 * Avg Reward is ==> -1495.379158361076\n",
      "Episode * 85 * Avg Reward is ==> -1492.1206750208707\n",
      "Episode * 86 * Avg Reward is ==> -1495.895954936918\n",
      "Episode * 87 * Avg Reward is ==> -1500.4607758178124\n",
      "Episode * 88 * Avg Reward is ==> -1503.418073952294\n",
      "Episode * 89 * Avg Reward is ==> -1498.672267604738\n",
      "Episode * 90 * Avg Reward is ==> -1494.5470166373743\n",
      "Episode * 91 * Avg Reward is ==> -1493.0619875250654\n",
      "Episode * 92 * Avg Reward is ==> -1492.9531429968895\n",
      "Episode * 93 * Avg Reward is ==> -1491.239059318419\n",
      "Episode * 94 * Avg Reward is ==> -1493.9954413527103\n",
      "Episode * 95 * Avg Reward is ==> -1486.7820161787433\n",
      "Episode * 96 * Avg Reward is ==> -1485.190563562467\n",
      "Episode * 97 * Avg Reward is ==> -1488.9181883157569\n",
      "Episode * 98 * Avg Reward is ==> -1488.038312316988\n",
      "Episode * 99 * Avg Reward is ==> -1490.1527630640708\n"
     ]
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    for timestep in range(np.sum(engine_lives)):\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Receive state and reward from environment.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2830f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55975731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
